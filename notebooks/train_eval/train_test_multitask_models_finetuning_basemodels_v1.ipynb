{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-hundred",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = os.path.join(os.path.abspath('../../'))\n",
    "repo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,repo_dir)\n",
    "import pridict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-pizza",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pth = os.path.join(repo_dir, 'dataset')\n",
    "data_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pridict.pridictv2.utilities import *\n",
    "from pridict.pridictv2.dataset import *\n",
    "from pridict.pridictv2.run_workflow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_available_cuda_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-diamond",
   "metadata": {},
   "source": [
    "### Generate datatensor partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsize=20\n",
    "outcome_suffix = 'clamped'\n",
    "include_MFE = False\n",
    "if include_MFE:\n",
    "    fsuffix = 'withMFE'\n",
    "else:\n",
    "    fsuffix = 'withoutMFE'\n",
    "    \n",
    "# use these to tune on 23K library HEK, K562\n",
    "tfolder_name = 'proc_v2'\n",
    "dsetnames_lst = ['HEK', 'K562']\n",
    "\n",
    "data_dir = create_directory(os.path.join(repo_dir, 'dataset', tfolder_name, f'align_{fsuffix}'))\n",
    "\n",
    "dtensor_partitions_lst: list[dict[int, dict[str, PartitionDataTensor]]] = []\n",
    "for outcome_name in dsetnames_lst:\n",
    "    fname = f'dpartitions_{outcome_name}_{outcome_suffix}_wsize{wsize}.pkl'\n",
    "    data_partitions =  ReaderWriter.read_data(os.path.join(data_dir, fname))\n",
    "    fname = f'dtensor_{outcome_name}_{outcome_suffix}_wsize{wsize}.pkl'\n",
    "    dtensor= ReaderWriter.read_data(os.path.join(data_dir, fname))\n",
    "    dtensor_partitions = generate_partition_datatensor(dtensor, data_partitions)\n",
    "    dtensor_partitions_lst.append(dtensor_partitions)\n",
    "    \n",
    "    \n",
    "dtensor_partitions_multidata: dict[int, list[dict[str, PartitionDataTensor]]] = {}\n",
    "for run_num in range(5):\n",
    "    dtensor_partitions_multidata[run_num] = []\n",
    "    for dtensor_partitions in dtensor_partitions_lst:\n",
    "        dtensor_partitions_multidata[run_num].append(dtensor_partitions[run_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtensor_partitions_multidata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-dylan",
   "metadata": {},
   "source": [
    "### Define model and experiment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-petite",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_gpu_map = {i:0 for i in range(len(data_partitions))}\n",
    "run_gpu_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-replication",
   "metadata": {},
   "source": [
    "### Specify which layers to finetune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_layernames = []\n",
    "\n",
    "for dsetname in dsetnames_lst:\n",
    "    for lname in ['seqlevel_featembeder',\n",
    "                  'decoder', \n",
    "                  'global_featemb_init_attn', \n",
    "                  'global_featemb_mut_attn', \n",
    "                  'local_featemb_init_attn',\n",
    "                  'local_featemb_mut_attn']:\n",
    "        trainable_layernames.append(f'{lname}_{dsetname}')\n",
    "        \n",
    "trainable_layernames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-brown",
   "metadata": {},
   "source": [
    "### Finetuning base models on 23K (HEK and K562) Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-origin",
   "metadata": {},
   "source": [
    "#### Use a base model pre-trained on Library 1 data (Mathis et al.) - see Figure 1 n in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "z_dim = 72\n",
    "num_hidden_layers = 2\n",
    "bidirection=True\n",
    "p_dropout = 0.15\n",
    "rnn_class = nn.GRU\n",
    "nonlin_func = nn.ReLU()\n",
    "l2_reg = 1e-5\n",
    "batch_size = 750\n",
    "num_epochs = 150\n",
    "# loss_func = 'KLDloss'\n",
    "loss_func = 'CEloss'\n",
    "# loss_func = 'Huberloss'\n",
    "trf_tup = [embed_dim, z_dim,\n",
    "           num_hidden_layers,\n",
    "           bidirection, \n",
    "           p_dropout,\n",
    "           rnn_class, nonlin_func,\n",
    "           l2_reg, batch_size, num_epochs]\n",
    "seqlevel_featdim = len(dtensor_partitions[0]['train'].pe_datatensor.seqlevel_feat_colnames)\n",
    "num_t_outcomes = 3\n",
    "default_outcomes = ['averageedited', 'averageunedited', 'averageindel']\n",
    "\n",
    "experiment_options = {'experiment_desc':'pe_rnn_distribution_multidata',\n",
    "                      'model_name':'PE_RNN_distribution_multidata',\n",
    "                      'annot_embed':8,\n",
    "                      'assemb_opt':'stack',\n",
    "                      'loader_mode':'cycle',\n",
    "                      'run_num':0,\n",
    "                      'fdtype':torch.float32,\n",
    "                      'wsize':wsize,\n",
    "                      'datasets_name':dsetnames_lst,\n",
    "                      'target_names': default_outcomes[:num_t_outcomes],\n",
    "                      'base_model_suffix':None,\n",
    "                      'separate_attention_layers':True,\n",
    "                      'separate_seqlevel_embedder':True,\n",
    "                      'seqlevel_featdim': seqlevel_featdim,\n",
    "                      'trainable_layernames': trainable_layernames,\n",
    "                      'num_outcomes':num_t_outcomes}\n",
    "mconfig, options = build_config_map(trf_tup, experiment_options, loss_func=loss_func)\n",
    "\n",
    "# provide the base model that will be used to fine-tune on the data\n",
    "# we will use base_90k (pretrained on Library 1) to finetune by specifying the folder name where the trained base model is found\n",
    "mfolder = 'exp_2023-06-02_09-49-21' # base_90k\n",
    "model_type = 'base_90k'\n",
    "trun = 1 # given that we have 5-fold training of base model we can specify which run to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-point",
   "metadata": {},
   "source": [
    "#### Use a base model pre-trained on Library 1 and Library-ClinVar data (Mathis et al. and Yu et al) - see Figure 1 n in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######\n",
    "# ## uncomment this cell to use this configuration to finetune 390k base model\n",
    "# #######\n",
    "\n",
    "# ## 390k model retuning\n",
    "# embed_dim = 128\n",
    "# z_dim = 72\n",
    "# num_hidden_layers = 2\n",
    "# bidirection=True\n",
    "# p_dropout = 0.15\n",
    "# rnn_class = nn.GRU\n",
    "# nonlin_func = nn.ReLU()\n",
    "# l2_reg = 1e-5\n",
    "# batch_size = 750\n",
    "# num_epochs = 150\n",
    "# # loss_func = 'KLDloss'\n",
    "# loss_func = 'CEloss'\n",
    "# # loss_func = 'Huberloss'\n",
    "# trf_tup = [embed_dim, z_dim,\n",
    "#            num_hidden_layers,\n",
    "#            bidirection, \n",
    "#            p_dropout,\n",
    "#            rnn_class, nonlin_func,\n",
    "#            l2_reg, batch_size, num_epochs]\n",
    "# seqlevel_featdim = len(dtensor_partitions[0]['train'].pe_datatensor.seqlevel_feat_colnames)\n",
    "# default_outcomes = ['averageedited', 'averageunedited', 'averageindel']\n",
    "# num_t_outcomes = 3\n",
    "# experiment_options = {'experiment_desc':'pe_rnn_distribution_multidata',\n",
    "#                       'model_name':'PE_RNN_distribution_multidata',\n",
    "#                       'annot_embed':8,\n",
    "#                       'assemb_opt':'stack',\n",
    "#                       'loader_mode':'cycle',\n",
    "#                       'run_num':0,\n",
    "#                       'fdtype':torch.float32,\n",
    "#                       'wsize':wsize,\n",
    "#                       'datasets_name':dsetnames_lst,\n",
    "#                       'target_names': default_outcomes[:num_t_outcomes],\n",
    "#                       'base_model_suffix':'HEKschwank',\n",
    "#                       'separate_attention_layers':True,\n",
    "#                       'separate_seqlevel_embedder':True,\n",
    "#                       'seqlevel_featdim': seqlevel_featdim,\n",
    "#                       'trainable_layernames': trainable_layernames,\n",
    "#                       'num_outcomes':num_t_outcomes}\n",
    "# mconfig, options = build_config_map(trf_tup, experiment_options, loss_func=loss_func)\n",
    "# ## the base model 390k to finetune\n",
    "# mfolder = 'exp_2023-08-26_20-58-14' # folder name where pretrained model is found\n",
    "# model_type = 'base_390k'\n",
    "# trun = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "repo_path = create_directory(os.path.join(os.path.abspath('../')))\n",
    "experiment_desc = experiment_options['experiment_desc']\n",
    "exp_dir = create_directory(os.path.join(repo_dir, 'experiments', experiment_desc))\n",
    "exp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-suffering",
   "metadata": {},
   "source": [
    "### Run training/fine-tuning on the 5-folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-brief",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mfolder = 'exp_2023-06-02_09-49-21' # base_90k **\n",
    "# model_type = 'base_90k'\n",
    "# trun = 1 \n",
    "\n",
    "# mfolder = 'exp_2023-08-26_20-58-14' # base_390k **\n",
    "# model_type = 'base_390k'\n",
    "# trun = 1\n",
    "\n",
    "\n",
    "trained_basemodel_dir = os.path.join(repo_dir, \n",
    "                                     'trained_models', \n",
    "                                     model_type,\n",
    "                                     mfolder,\n",
    "                                     'train_val')\n",
    "config_map = (mconfig, options)\n",
    "trmodels_dir_lst = []\n",
    "for base_model_run in [trun]: \n",
    "    time_stamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    tr_val_dir = create_directory(f'exp_{time_stamp}', exp_dir)\n",
    "    state_dict_dir = os.path.join(trained_basemodel_dir, \n",
    "                                  f'run_{base_model_run}',\n",
    "                                 'model_statedict')\n",
    "    trmodels_dir_lst.append(tr_val_dir)\n",
    "    print('basemodel_run:', base_model_run)\n",
    "    print('state_dict_dir:', state_dict_dir)\n",
    "    print('tr_val_dir:', tr_val_dir)\n",
    "    print()\n",
    "    tune_trainval_run(dtensor_partitions_multidata,\n",
    "                      config_map, \n",
    "                      tr_val_dir, \n",
    "                      state_dict_dir, \n",
    "                      run_gpu_map, \n",
    "                      num_epochs=num_epochs) # change num_epochs if you want to do a `dry test` (i.e. fast check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-freeware",
   "metadata": {},
   "source": [
    "### Run train models on test set of each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_map = (mconfig, options)\n",
    "for tr_val_dir in trmodels_dir_lst:\n",
    "    print('evaluating modeldir:', tr_val_dir)\n",
    "    test_multidata_run(dtensor_partitions_multidata,\n",
    "                   config_map, \n",
    "                   tr_val_dir, \n",
    "                   tr_val_dir, \n",
    "                   run_gpu_map, \n",
    "                   num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-capitol",
   "metadata": {},
   "source": [
    "### Models' evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsetnames = dsetnames_lst\n",
    "for dsettype in ('train', 'validation', 'test'):\n",
    "    print(f'--- {dsettype} ---')\n",
    "    for outcome_name in ['averageedited', 'averageunedited', 'averageindel']:\n",
    "        out = build_performance_multidata_dfs(tr_val_dir, 5, dsettype, outcome_name, 'continuous', dsetnames)\n",
    "        for i_data, dsetname in enumerate(dsetnames):\n",
    "            display(out[i_data])\n",
    "            \n",
    "    print('*'*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-buyer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
